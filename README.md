# ğŸ§­ From Curiosity to Discovery: A Reinforcement Learning Approach to Treasure Hunting

---

## ğŸ† Project Overview

**From Curiosity to Discovery** is an interactive AI project that demonstrates how a reinforcement learning (RL) agent learns to survive and succeed in a **Treasure Hunt environment** filled with static and dynamic traps, lifelines, and hidden treasures.  

Built using **Gymnasium** and **Pygame**, this simulation bridges **game design** and **artificial intelligence** â€” showing how agents can adapt in uncertain and partially observable worlds.

---

## ğŸ® Environment Description

The environment is a **2D grid world** where the agent explores to collect treasures while avoiding traps and managing lifelines.

| Element | Description | Reward / Penalty |
|----------|--------------|------------------|
| ğŸ§± **Wall** | Blocks movement | â€“2 |
| ğŸŸ« **Road** | Safe path (pitch brown) | â€“1 per move |
| ğŸ’ **Treasure (â™¢)** | Increases score | +30 |
| ğŸ **Final Treasure (â™¢)** | Increases score | +70 |
| â˜ ï¸ **Static Trap (âŠ—)** | Old hidden traps, 50% chance of life loss | â€“6 |
| âš¡ **Dynamic Trap (Hunters)** | Appears randomly; always decreases life | â€“12 |
| â¤ï¸ **Lifeline (â™¡)** | Restores 1 life | +5 |
| ğŸ§ **Agent** | Learns via RL | â€” |

- **Visibility:** limited to 4 cells; walls block view.  
- **Dynamic traps** appear every few steps to simulate unpredictable danger.  
- The agent starts with **3 lifelines** and can gain up to **5**.  

---

## ğŸ§  Algorithms Implemented

| Algorithm | Type | Description |
|------------|------|--------------|
| **DQ-N** | - | Learns from expected future rewards (exploitative) |

Algorithms are trained and evaluated under the same environment for performance comparison.

---

## âš™ï¸ Project Structure


TreasureHuntRL/
â”‚
â”œâ”€â”€ env/                                # ğŸŒ Environment Module
â”‚   â”œâ”€â”€ treasure_env.py                 # Main Pygame + Gymnasium environment
â”‚   â”œâ”€â”€ map_layouts/                    # Different maps or levels
â”‚   â”‚   â”œâ”€â”€ map_easy.json
â”‚   â”‚   â”œâ”€â”€ map_medium.json
â”‚   â”‚   â””â”€â”€ map_hard.json
â”‚   â”œâ”€â”€ assets/                         # Game icons and visuals
â”‚   â”‚   â”œâ”€â”€ wall.png
â”‚   â”‚   â”œâ”€â”€ road.png
â”‚   â”‚   â”œâ”€â”€ treasure.png
â”‚   â”‚   â”œâ”€â”€ trap_static.png
â”‚   â”‚   â”œâ”€â”€ trap_dynamic.png
â”‚   â”‚   â”œâ”€â”€ heart.png
â”‚   â”‚   â””â”€â”€ agent.png
â”‚   â””â”€â”€ __init__.py
â”‚
â”œâ”€â”€ agents/                             # ğŸ§  RL Agents
â”‚   â”œâ”€â”€ dqn_agent.py                   # Only agent
â”‚   â”œâ”€â”€ base_agent.py                   # Common utilities (optional)
â”‚   â””â”€â”€ __init__.py
â”‚
â”œâ”€â”€ training/                           # âš™ï¸ Training Scripts
â”‚   â”œâ”€â”€ train_dqn.py
â”‚   â””â”€â”€ hyperparams.json                # Tunable parameters (alpha, gamma, epsilon)
â”‚
â”œâ”€â”€ analysis/                           # ğŸ“Š Result Analysis
â”‚   â”œâ”€â”€ evaluation.py                  # Graphs and metrics comparison
â”‚   â”œâ”€â”€ logs/                           # Episode logs (rewards, steps, lifelines)
â”‚   â”‚   â”œâ”€â”€ dqn_training_hard.csv
â”‚   â”‚   â”œâ”€â”€ dqn_training_medium.csv
â”‚   â”‚   â””â”€â”€ dqn_training_easy.csv
â”‚   â””â”€â”€ plots/                          # Generated plots
â”‚       â”œâ”€â”€ rewards_vs_episodes.png
â”‚       â”œâ”€â”€ steps_vs_episodes.png
â”‚       â””â”€â”€ traps_vs_treasures.png
â”‚
â”œâ”€â”€ test_env_run.py
â”œâ”€â”€ manual_run.py
â”œâ”€â”€ test_dqn.py                         # ğŸš€ Central launcher for the project
â”œâ”€â”€ requirements.txt                    # Dependencies (Gymnasium, Pygame, Numpy, Matplotlib)
â”œâ”€â”€ README.md                           # Project overview + usage guide
â””â”€â”€ .gitignore                          # To ignore unnecessary files in repo


---

